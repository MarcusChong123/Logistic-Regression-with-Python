# -*- coding: utf-8 -*-
"""Logistic Regression_Tutorial.ipynb

Automatically generated by Colaboratory.

"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# Provided you are running IPython, the %matplotlib inline will make your plot outputs appear and be stored within the notebook.

import pandas as pd #numerical analysis, for manipulating numerical tables and time series
import numpy as np #adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions

import seaborn as sns #Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.
import matplotlib.pyplot as plt #plotting engine

# %config InlineBackend.figure_format = 'retina' #make your chart/graph looks sharper
sns.set() #start seaborn engine

from google.colab import files
uploaded = files.upload()

# Step 2: Load the data
df = pd.read_csv("Turnover_train.csv")

df.tail()

# Step 3: Explore the data
df.info()

df.describe()

# Step 4: Exploratory Data Analysis and Data Wrangling
sns.set(rc={'figure.figsize':(12,10)}) #widthxheight
sns.set_context("talk", font_scale=1) #fontscale
    
sns.heatmap(df.corr(), cmap='Reds', annot=True) #correlationshiptable with heatamap

# Draw a count plot that shows the count of employees per department

ax = sns.countplot(df['role']) #do a count plot wrt role
loc, labels = plt.xticks()
ax.set_xticklabels(labels, rotation=90); #set x label, rotate it by 90 deg

# Configure the dataframe, the x axis, and hue
sns.countplot(x="salary", hue="role", data=df, palette='Set2') #palette is color set, plot salary, factor by role

sns.countplot(x="salary", hue="left", data=df ) #plot salary, factor by left

# Visualize the effect of number of projects for the staff who has left

sns.countplot(x="number_project", hue="left", data=df ); #plot num_project, factor by left

# Factor Plot/Distribution plot
import matplotlib.pyplot as plt
fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(12,4)) #widthxheight, 1 row, 3 columns (satis, last_eval, avg_month)

#norm_hist -> If True, the histogram height shows a density (normalize value) rather than a count
#kde -> Whether to plot a gaussian kernel density estimate

sns.distplot(df['satisfaction_level'], norm_hist=False, kde=False, ax=ax1);
sns.distplot(df['last_evaluation'], norm_hist=False, kde=False, ax=ax2);
sns.distplot(df['average_monthly_hours'], norm_hist=False, kde=False, ax=ax3);

df.head()

df.drop(['names'], 1, inplace=True)
obj_df = df.select_dtypes(include=['object']).copy()

from sklearn import preprocessing
lb_make = preprocessing.LabelEncoder()
for col in obj_df.columns.values:
  df[f'{col}_new'] = lb_make.fit_transform(df[f'{col}'])

df.head()

df.drop(['role', 'salary'], 1, inplace=True)

df.head()

df.info()

import statsmodels.formula.api as smf
glm = smf.logit(formula = 'left ~ satisfaction_level + number_project + last_evaluation + average_monthly_hours + exp_in_company + work_accident + promotion_last_5years + role_new', data = df).fit()
glm.params

glm.summary()

# Step 5: Train the model

from sklearn.linear_model import LogisticRegression
from sklearn import preprocessing

feature_cols = ['satisfaction_level','number_project', 'last_evaluation', 'average_monthly_hours', 'exp_in_company', 'work_accident', 'promotion_last_5years', 'role_new']

X = df[feature_cols] #these are the independent variables
X = preprocessing.scale(X)
y = df['left'] #dependent variable

from sklearn.metrics import accuracy_score

logmodel = LogisticRegression()
logmodel.fit(X,y) #model calculation

logmodel.score(X, y)

df_test = pd.read_csv("Turnover_test.csv")

df_test.head()

df_test2 = df_test.drop(['id', 'names'], 1)
obj_df_test = df_test2.select_dtypes(include=['object']).copy()

from sklearn import preprocessing
lb_make = preprocessing.LabelEncoder()
for col in obj_df_test.columns.values:
  df_test2[f'{col}_new'] = lb_make.fit_transform(df_test2[f'{col}'])

df_test2.drop(['role', 'salary'], 1, inplace=True)

df_test2.head()

X_test = df_test2[feature_cols]
X_test = preprocessing.scale(X_test)

predictions = logmodel.predict(X_test) #make predictions based on test dataa

df_test['left_predict'] = predictions #add "left_predict" column to the imported test data

df_test.head(20)

